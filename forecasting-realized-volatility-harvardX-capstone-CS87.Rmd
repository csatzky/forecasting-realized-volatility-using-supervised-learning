---
title: "Forecasting Realized Volatility in the EUR/USD Currency Pair"
subtitle: "Capstone Project in the HarvardX Data Science Program"
author: "[Christian Satzky](https://www.linkedin.com/in/christian-satzky/), edX ID CS87"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    keep_tex: yes
    number_sections: true
urlcolor: blue
---

\newpage
\tableofcontents
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      tidy=TRUE,
                      warning = FALSE, message = FALSE)
library(kableExtra)
library(gridExtra)
```

# Acknowledgement

This analysis is for partly fulfillment of the 'Capstone' module in the [HarvardX Data Science](https://www.edx.org/professional-certificate/harvardx-data-science) program, an online learning initiative by Harvard University.

# Background Information: Financial Volatility

Financial market participants trade assets such as stocks, currencies, bonds and commodities. The prices of these assets are continuously changing. These price fluctuations are measured in terms of 'volatility'. 

Mathematically, volatility refers to the _standard deviation_ of the asset's _price changes_. The greater the volatility of an asset, the more _uncertainty_ there is regarding the asset's price developments. Consider two assets, A and B. In the future, both assets _on average_ yield \$100 in return. Assume that for asset A this return is certain. However, for asset B, the return is going to be either \$200 or \$0 with equal probability. Observe that asset A yields the same _average_ return as asset B, but without the risk of getting nothing. If both assets were priced equally, the _rational investor_ would always choose asset A over asset B, as it has the same average return but without any risk. In this example, asset A has a volatility of 0, while asset B has a volatility of 100: 

$$\sigma = \sqrt{\sum_{i=1}^{N}{\textrm{p}_i (x_i - \mu})^2} = \sqrt{0.5 * (200 - 100)^2 + 0.5 * (0 - 100)^2} = 100$$

The greater the uncertainty regarding an asset's price developments, the greater the risk to the investor. 

# Introduction and Objective

Accurate _forecasting_ of volatility is crucial for the pricing of financial derivatives, portfolio allocation and effective risk management. The goal of this project is to find machine-learning methods that are best suited for volatility forecasting.

For this project, [SHARELAB LIMITED](https://sharelab.com) provided data containing daily _realized volatility_ (RV) of EUR/USD currency quotes. The data provided consists of a `r text_spec("training", color="black", background="#EEEEEE")` set (data from 1/1/2010 to 12/31/2019) and a `r text_spec("validation", color="black", background="#EEEEEE")` set (data from 1/1/2020 to 7/31/2020). The objective of this project is to predict $\textrm{RV}_{t+1}$ (i.e. EUR/USD realized volatility for date $t + 1$) as accurately as possible on the pseudo out-of-sample `r text_spec("validation", color="black", background="#EEEEEE")` set.

More specifically, any analysis and modeling is strictly performed on the `r text_spec("training", color="black", background="#EEEEEE")` dataset, while the `r text_spec("validation", color="black", background="#EEEEEE")` subset is used only to assess the final model's performance per machine-learning method considered. The performance measure is the root mean square error (RMSE).

With consent of SHARELAB LIMITED, the data is publicly available in the [*.Rdata file on GitHub](https://).

^[Note: In academic literature, the GARCH model (Bollerslev, 1986) is widely used to capture the conditional heteroskedascity (time-dependent standard deviation) of financial time series. Under the GARCH model the autocorrelation function of the squared asset returns decays at an exponential rate (He and Teräsvirta, 1997). This "short memory" property of the GARCH model contradicts commonly observed long range dependencies (e.g. Mandelbrot and Taqqu, 1979). In the provided dataset, we are presented with a problem to forecast volatility given past volatility measures as opposed to squared price returns, and the forecast horizon spawns across several months. This calls for a different approach of modeling volatility as described in this report.]

# Executive Summary

The final model achieves an **RMSE of 0.8507** on the `r text_spec("validation", color="black", background="#EEEEEE")` dataset – a significant improvement from the RMSE required by HarvardX to score _full_ marks (0.8649).

The chosen model is a linear combination of variables capturing the following effects:

* mean rating
* (penalized) movie-to-movie variability ('movie bias')
* (penalized) user-to-user variability ('user bias')
* genre-to-genre variability ('genre bias')
* (penalized) user-specific genre-to-genre variability ('user-specific genre bias')
* time variability ('seasonal bias')

The movie bias, user bias and user-specific genre bias have the greatest impact on the model. The general genre bias and the seasonal bias, albeit being highly significant, only have a slight impact on predictions.

All code is _fully reproducible_ and all steps that resulted in the final model are documented below. For the complete code to produce all interim results, please look into the *.Rmd file directly. In the PDF version, most chunks of code are not displayed for an improved reading experience.

# Modeling Approach

To prevent potential overtraining, the `r text_spec("training", color="black", background="#EEEEEE")` dataset is first split into `r text_spec("train", color="black", background="#EEEEEE")` and `r text_spec("test", color="black", background="#EEEEEE")` subsets. Using the `r text_spec("train", color="black", background="#EEEEEE")` subset, the data is explored using summary statistics and data visualization. In the process, some variables are transformed and additional predictors are created. 

Secondly, different machine-learning methods are applied to estimate the conditional expectation $E[{\textrm{RV}_{t+1}|X}]$. For each method, any model variants and any tuning of parameters is done using the `r text_spec("train", color="black", background="#EEEEEE")` and `r text_spec("test", color="black", background="#EEEEEE")` sets only. 

Finally, using the RMSE information on the `r text_spec("test", color="black", background="#EEEEEE")` subset, _one_ final model _per algorithm_ is chosen to be evaluated on the `r text_spec("validation", color="black", background="#EEEEEE")` set. Considering the RMSE performance on the `r text_spec("validation", color="black", background="#EEEEEE")` set of each method's final model, it is possible to compare the suitability of different algorithms to forecast volatility for the EUR/USD currency.


```{r load-dataset, include = FALSE}
# load EUR/USD volatility dataset sponsored by sharelab.com
# NOTE: all code is designed to run on R 3.6.X (Windows machine)
# code runs successfully on personal computer (Intel Core i7-7500U, 16GB RAM) 

# load/install libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# load 'training' and 'validation' sets
load("./data/EURUSD_realized_volatility.RData")


# download data from GitHub 
dl <- "EURUSD_realized_volatility.zip"
download.file("https://github.com/csatzky/eurusd-realized-volatility-forecasting/raw/main/data/EURUSD_realized_volatility.zip", dl)

# unzip and load local data (relative paths)
files <- unzip(dl)
training <- fread(files[grepl("training",files)], sep=",", header=TRUE, stringsAsFactors=FALSE)
validation <- fread(files[grepl("validation",files)], sep=",", header=TRUE, stringsAsFactors=FALSE)

# load downloaded dataset
load(paste0("data/",dl))

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
```

```{r create-train-test-sets, echo=TRUE}
# <-- code starts after running 'Create edx set, validation set (final hold-out test set)' by HarvardX -->

# increase memory limit on Windows machine
invisible(memory.limit(size = 35000))

# load libraries
library(tidyverse)
library(caret)
library(data.table)

# for reproducibility
set.seed(100, sample.kind="Rounding")

# split 'edx' into 'train' and 'test' sets, with p(test)=0.15
test_ind <- createDataPartition(y = edx$rating, times = 1, p = 0.15, list = FALSE)
test <- edx[test_ind,]
train <- edx[-test_ind,]

# ensure that 'test' contains the same userIds, movieIds, genres
# [note: thereby losing few observations on test set]
test <- test[which(test$movieId %in% train$movieId),]
test <- test[which(test$userId %in% train$userId),]
test <- test[which(test$genres %in% train$genres),]

# calculate percentage of train observations vs. edx observations
percent_train <- round(nrow(train)/nrow(edx) * 100, 1)

# calculate percentage of test observations vs. edx observations
percent_test <- round(nrow(test)/nrow(edx) * 100, 1) # still roughly 15%
```


# Data Exploration

Data exploration and any modeling is strictly performed on the `r text_spec("train", color="black", background="#EEEEEE")` set, which contains roughly `r percent_train`% of the observations from the given `r text_spec("edx", color="black", background="#EEEEEE")` dataset. The `r text_spec("train", color="black", background="#EEEEEE")` and `r text_spec("test", color="black", background="#EEEEEE")` sets have overlapping values for `r text_spec("movieId", color="black", background="#EEEEEE")`, `r text_spec("userId", color="black", background="#EEEEEE")` and `r text_spec("genres", color="black", background="#EEEEEE")` variables, thereby losing `r nrow(edx) - nrow(train) -nrow(test)` observations for training.

```{r summary-stats, echo=TRUE}
# retrieve variable names, classes, unique number of values and value ranges
var_info <- rbind(
  class = sapply(train, class),
  n_unique_values = sapply(train,
    FUN = function(col){
    length(unique(col))}),
  val_range_from = sapply(train, range)[1,],
  val_range_to = sapply(train, range)[2,]
)

# display latex-style table
kable(var_info, booktabs = TRUE) %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(position = "center")
```

From the table above, we see that:

* the variable $Y_{i,u}$, `r text_spec("rating", color="black", background="#EEEEEE")`, only has 10 unique values, ranging from 0.5 to 5
* there are roughly 6 times more users than there are movies
* there are `r var_info["n_unique_values","genres"]` different movie categories ('`r text_spec("genres", color="black", background="#EEEEEE")`'). This number appears large and it might be sensible to narrow it down
* values of the `r text_spec("timestamp", color="black", background="#EEEEEE")` variable have little interpretability and e.g. quarter / month / week of the year should be extracted for further investigation


## Data Wrangling and Feature Extraction

For data exploration purposes, the variable `r text_spec("timestamp", color="black", background="#EEEEEE")` is used to extract `r text_spec("quarter", color="black", background="#EEEEEE")`, `r text_spec("month", color="black", background="#EEEEEE")`, and `r text_spec("week", color="black", background="#EEEEEE")`.

```{r extract-time-dimensions, echo=TRUE}
library(lubridate)

# extract 'quarter', 'month', and 'week' from 'timestamp' variable
train[, timestamp := as_datetime(timestamp)]
train[,(c("quarter", "month", "week")) := list(quarter(timestamp), month(timestamp), week(timestamp))]
```

In addition, it is clear that having `r var_info["n_unique_values","genres"]` different movie categories in the `r text_spec("genres", color="black", background="#EEEEEE")` variable is not practical. A sample of unique values in the `r text_spec("genres", color="black", background="#EEEEEE")` variable shows that each observation is a character string combining different movie genres:

```{r genres-sample}
# for reproducibility
set.seed(100, sample.kind="Rounding")

# show sample of unique movie categories
sample(unique(train$genres), size = 5, replace = FALSE)
```

Using `r text_spec("str_split()", color="black", background="#EEEEEE")` we can extract all single genre categories.
```{r genres-wrangling, echo=TRUE}
# exclude/change special characters from 'genres' variable
train[, genres := str_replace_all(genres, "[-()]", "")]
train[, genres := str_replace_all(genres, " ", "")]

# extract all single genres
single_genres <- str_split(train$genres, "\\|")
single_genres <- unique(unlist(single_genres))

single_genres
```

From the above, we see that the `r var_info["n_unique_values","genres"]` unique values in the `r text_spec("genres", color="black", background="#EEEEEE")` variable is a combination of only `r length(single_genres)` single genre categories. Next, let's check how many movies are associated with each single genre.


## Stratification by Movie Genre

Below, I stratify movie genre by number of associated movies and average movie rating.

```{r stratify-rating-by-genre}
# for each single genre, compute number of associated movies, mean and sd of average ratings
avg_rating_per_genre <- sapply(single_genres, 
       FUN = function(genre){
         # get movieIds associated with current genre
         genre_movies <- train[str_detect(genres, pattern = genre), movieId]
         
         # get number of movies associated with current genre
         n_movies <- length(unique(genre_movies))
         
         # calculate the mean rating for each movie associated with the current genre
         mean_rating_per_movie <- train[movieId %in% genre_movies, mean(rating), movieId]
         
         # calculate the mean value of the average movie ratings
         mean_genre_movies <- mean(mean_rating_per_movie$V1)
         
         # calculate sd of the average movie ratings
         sd_genre_movies <- sd(mean_rating_per_movie$V1)
         
         # output
         data.frame(mean_rating = mean_genre_movies, sd_rating =  sd_genre_movies, n_movies = n_movies)
         }
      )

# plot number of movies and average ratings per genre
genre_df <- data.frame(genre = colnames(avg_rating_per_genre), n_movies = unlist(avg_rating_per_genre[3,]), mean_rating = unlist(avg_rating_per_genre[1,]), sd_rating = unlist(avg_rating_per_genre[2,]))
genre_df %>%
  mutate(genre=reorder(genre, -n_movies)) %>%
  ggplot(aes(x=genre, y=n_movies, fill=mean_rating, labels=mean_rating)) +
  theme(axis.text.x = element_text(angle=-90, hjust=0, vjust=0.2)) +
  scale_y_sqrt() +
  geom_col() +
  ggtitle("Number of Movies per Associated Genre") +
  xlab("Associated Genre") +
  ylab("N Movies")
```

The plot above shows that:

* a large number of movies is associated with genres `r paste(tail(genre_df[order(genre_df$n_movies),],2)$genre, collapse=" and ")`
* few movies are associated with genres `r paste(head(genre_df[order(genre_df$n_movies),],2)$genre, collapse=" and ")`
* movies associated with genre `r paste(head(genre_df[order(genre_df$mean_rating),],1)$genre)` have the worst average rating
* movies associated with genre `r paste(tail(genre_df[order(genre_df$mean_rating),],1)$genre)` have the best average rating
* there is no apparent relationship between 'popularity' of genre and average movie rating

Note that the genre with the best ratings has relatively few movies. Below, I am plotting the 95% confidence intervals of average movie ratings by associated genre.

```{r genre-rating-CIs}
# for each single genre, get associated movies and calculate the mean rating per movie
avg_movie_ratings_per_genre <- lapply(single_genres, 
       FUN = function(genre){
         # get movieIds associated with current genre
         genre_movies <- train[str_detect(train$genres, pattern=genre)]$movieId
         
         # calculate the mean rating for each movie associated with the current genre
         mean_rating_per_movie <- train[movieId %in% genre_movies, mean(rating), movieId]
         
         # output
         data.frame(genre = rep(genre,nrow(mean_rating_per_movie)), avg_rating_per_movie = mean_rating_per_movie$V1, n_movies = nrow(mean_rating_per_movie), sd=sd(mean_rating_per_movie$V1))
         }
      )

avg_movie_ratings_per_genre <- rbindlist(avg_movie_ratings_per_genre)
mean_ratings_per_genre <- avg_movie_ratings_per_genre[, mean(avg_rating_per_movie), genre]
mean_avg_movie_ratings_per_genre <- unique(merge(mean_ratings_per_genre[,.(genre, mean_movie_ratings=V1)], avg_movie_ratings_per_genre[,.(genre, n_movies, sd)], all.x=TRUE, by="genre"))

# plot CIs of average movie ratings by associated genre:
mean_avg_movie_ratings_per_genre %>%
  mutate(ci = sd/sqrt(n_movies) * qnorm(0.975)) %>%
  mutate(genre = reorder(genre, -mean_movie_ratings)) %>%
  ggplot(aes(x=genre, y=mean_movie_ratings)) +
  theme(axis.text.x = element_text(angle=-90, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_movie_ratings + ci, ymin = mean_movie_ratings - ci)) +
  ggtitle("95% CIs of Average Movie Ratings") +
  xlab("Associated Genre") +
  ylab("95% CIs of Rating")
```
From the plot above, we observe:

* movies associated with the Film-Noir genre have the greatest average movie rating
* movies associated with the Horror genre have the lowest average movie rating
* both 95% confidence intervalls for genres Film-Noir and Horror do not overlap with any other genre
* the Drama genre has by far the shortest confidence interval, this is partly because it is associated with the largest number of movies
* the IMAX genre has the largest confidence interval
* note that 'nogenreslisted' is only associated with one movie and therefore has no standard deviation of mean ratings that can be used to compute a CI


## Stratification by Time Periods

In this sub-section, I am exploring potential time effects (seasonality) in the data. Below, I am plotting 95% confidence intervals for mean movie ratings stratified by quarter, month and week.
```{r stratification-time-periods}
## stratify by quarter
# stratify average ratings by quarter. Calculate 95% CIs
dt_mean <- train[,mean(rating),quarter]
dt_sd <- train[,sd(rating),quarter]
dt_n <- train[,.N,quarter]
dt <- merge(dt_mean[,.(quarter, mean_rating = V1)], dt_sd[,.(quarter, sd = V1)], by="quarter")
dt <- merge(dt, dt_n, by="quarter")

# plot CIs of average movie ratings by quarter:
p1 <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=quarter, y=mean_rating)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_rating + ci, ymin = mean_rating - ci)) +
  ggtitle("95% CIs of Mean Rating by Quarter") +
  ylab("") +
  theme(plot.title = element_text(size=12))


## stratify by month
# stratify average ratings by month. Calculate 95% CIs
dt_mean <- train[,mean(rating),month]
dt_sd <- train[,sd(rating),month]
dt_n <- train[,.N,month]
dt <- merge(dt_mean[,.(month, mean_rating = V1)], dt_sd[,.(month, sd = V1)], by="month")
dt <- merge(dt, dt_n, by="month")

# plot CIs of average movie ratings by month:
p2 <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=month, y=mean_rating)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_rating + ci, ymin = mean_rating - ci)) +
  ggtitle("95% CIs of Mean Rating by Month") +
  ylab("") +
  theme(plot.title = element_text(size=12))


## stratify by week
# stratify average ratings by week. Calculate 95% CIs
dt_mean <- train[,mean(rating),week]
dt_sd <- train[,sd(rating),week]
dt_n <- train[,.N,week]
dt <- merge(dt_mean[,.(week, mean_rating = V1)], dt_sd[,.(week, sd = V1)], by="week")
dt <- merge(dt, dt_n, by="week")

# plot CIs of average movie ratings by week:
p3 <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=week, y=mean_rating)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_rating + ci, ymin = mean_rating - ci)) +
  ggtitle("95% CIs of Mean Rating by Week") +
  ylab("") +
  theme(plot.title = element_text(size=12))


# use smoothing for weekly plot
p4 <- dt %>%
  ggplot(aes(x=week, y=mean_rating)) +
  geom_point(size = 0.8) +
  geom_smooth(span=0.3, se = FALSE) +
  ggtitle("Mean Rating by Week (LOESS)") +
  ylab("") +
  theme(plot.title = element_text(size=12))

grid.arrange(p1, p2, p3, p4, ncol=2)
```
As seen in the above plots;

* there seem to be some seasonal effects in the data
* in the last quarter of the year, movies generally seem to receive better ratings
* in April movies tend to receive better ratings than in March and May
* in October, movie ratings seem to be greatest
* in the weekly plot, another local peak seems to be around the end of June

In conclusion, the data suggests that `r text_spec("rating", color="black", background="#EEEEEE")` has some time dependencies.


# Modeling

Now that the data is sufficiently explored and additional features are created, in this section I am fitting different models on the `r text_spec("train", color="black", background="#EEEEEE")` set and check for an RMSE estimate on the `r text_spec("test", color="black", background="#EEEEEE")` set.

## The Benchmark Model

The benchmark model is simply the average rating, without using any of the other variables. It can be directly evaluated on the `r text_spec("test", color="black", background="#EEEEEE")` set.

$$Y_{m,u} = \mu + \epsilon_{m,u}$$

```{r benchmark-model}
# calculate mean rating
benchmark_model <- mean(train$rating)

# create function to calculate RMSE
get_rmse <- function(y_hat, y){
  round(sqrt(mean((y-y_hat)**2)),10)
}

rmse_benchmark <- get_rmse(benchmark_model, test$rating)
```

The benchmark model achieves an RMSE of `r round(rmse_benchmark,5)` on the `r text_spec("test", color="black", background="#EEEEEE")` set.

## Multivariate Model

Other features in the data set are related to movies, users, genres (type of movie) and time effects. In the following, I am incorporating each of these effects. Each model will be evaluated on the `r text_spec("test", color="black", background="#EEEEEE")` set and new effects will be added consecutively to the best performing model.

### Movie-to-Movie Variability

It is reasonable that a specific movie is perceived as 'good' or 'bad' by many users who rate the movie. This variability is captured by $\beta_m$, i.e. the _mean residual_ of the benchmark model for each movie $m$.

$$Y_{m,u} = \mu + \beta_m + \epsilon_{m,u}$$
As there are `r var_info["n_unique_values","movieId"]` unique movies in the data set, it is too computationally costly to add dummy variables or using the `r text_spec("lm()", color="black", background="#EEEEEE")` function to fit the model. Instead, I am using vectorization to stratify the benchmark model's residuals by `r text_spec("movieId", color="black", background="#EEEEEE")` to obtain an estimate for $\beta_m$. The residuals of the benchmark model are obtained by substracting the _mean rating_ from `r text_spec("rating", color="black", background="#EEEEEE")`, i.e. $\epsilon_{m,u} = Y_{m,u} - \bar Y$. 

```{r beta-m-estimation}
# compute residuals (epsilon) from benchmark model
residuals_benchmark <- train$rating - benchmark_model

# estimate beta_m on train set
train[, residuals_benchmark := residuals_benchmark] # add centered y variable to train set
mean_rating_per_movie <- train[, mean(residuals_benchmark), movieId] # calculate mean error per movieId
train <- merge(train, mean_rating_per_movie[,.(movieId, beta_m_hat = V1)], by="movieId", all.x=TRUE) # add beta_m_hat to train set

# fit movie effect model and add residuals to train set
y_hat_train <- benchmark_model + train$beta_m_hat
train[, residuals_movie_effect_model := rating - y_hat_train]

# use trained movie effect model to predict y variable on train set
test <- merge(test, unique(train[,.(beta_m_hat, movieId)]), all.x=TRUE, by = "movieId") # add estimated beta_m to test set
y_hat_test <- benchmark_model + test$beta_m_hat # predict test set ratings

# obtain RMSE on test set
rmse_movie_effect <- get_rmse(y_hat = y_hat_test, y = test$rating)

# summarize models' performance
models <- data.frame(Model=c("Benchmark", "Movie Effect"),
                            Variables = c("$\\mu$","$\\mu, \\beta_m$"), 
                            RMSE = c(rmse_benchmark, rmse_movie_effect))

names(models)[3] <- "$\\text{RMSE}_{\\text{test}}$"
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

We see that the movie effects model improves RMSE by roughly 10% in comparison to the benchmark model.

### User-to-User Variability

It is likely that different individuals _generally_ rate movies differently ('user bias'). Hence it is sensible to account for rating variability across `r text_spec("userId", color="black", background="#EEEEEE")`s.

I am estimating $\beta_u$ by stratifying the residuals of the movie effect model by `r text_spec("userId", color="black", background="#EEEEEE")`. The estimation is performed analogously to the movie effect model, but instead of stratifying the benchmark model's residuals, I work from the movie effect model's residuals. The estimated model is of the form:

$$Y_{m,u} = \mu + \beta_m + \beta_u + \epsilon_{m,u}$$
```{r user-effect-model}
# estimate beta_u on train set, using prior model's residuals
mean_residual_per_user <- train[, mean(residuals_movie_effect_model),  userId] # calculate mean error per userId
train <- merge(train, mean_residual_per_user[,.(userId, beta_u_hat = V1)], by="userId", all.x=TRUE) # add beta_u_hat to train set

# fit movie and user effect model and add residuals to train set
y_hat_train <- benchmark_model + train$beta_m_hat + train$beta_u_hat
train[, residuals_m_u_effect_model := rating - y_hat_train]

# use trained movie and user effect model to predict y variable on train set
test <- merge(test, unique(train[,.(beta_u_hat, userId)]), all.x=TRUE, by = "userId") # add estimated beta_u to test set
y_hat_test <- benchmark_model + test$beta_m_hat + test$beta_u_hat # predict test set ratings

# note that range(y_hat_test) has values outside of possible rating range [0.5, 5]. Any predictions outside this range are truncated
y_hat_test[y_hat_test > 5] <- 5
y_hat_test[y_hat_test < 0.5] <- 0.5

# obtain RMSE on test set
rmse_m_u_effect <- get_rmse(y_hat = y_hat_test, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "Movie, User Effects",
                     Variables = "$\\mu, \\beta_m, \\beta_u$", 
                     RMSE = rmse_m_u_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

Accounting for user-to-user variability further improves RMSE predictions by `r round(100*abs(models[3,3]/models[2,3] - 1))`% in comparison to the movie effect model.

### Genre-to-Genre Variability

In the data exploration part, I investigated the confidence intervals of mean rating per associated movie genre. For instance, the genre 'horror' is rated worst and its 95% confidence interval (CI) is not overlapping with any other genre. This information implies that a movie associated with a specific genre _generally_ receives better ratings than a movie associated with other genres. This genre-to-genre variability is considered in this section.

After having accounted for the movie-to-movie and user-to-user variability in the data, it is reasonable to look at the 95% CIs of mean error per genre again.

```{r plot-95CIs-genre-effect}
# prepare genre table
genre_dt <- as.data.table(genre_df)
genre_dt[, genre := as.character(genre)]
genre_dt <- genre_dt[order(-sd_rating), ] # add user-genre effect with greatest variability first

# for each single genre, get associated movies and calculate the mean residual (of movie & user effects model) per movie
mean_residual_per_genre <- lapply(genre_dt$genre, 
       FUN = function(genre){
         # filter for movies associated with current genre. Calculate mean residual of the movie_user_effect_model
         mean_residual_for_genre_movies <- train[str_detect(train$genres, pattern=genre), mean(residuals_m_u_effect_model)]
         
         # filter for movies associated with current genre. Calculate SD residual of the movie_user_effect_model
         sd_residual_for_genre_movies <- train[str_detect(train$genres, pattern=genre), sd(residuals_m_u_effect_model)]
         
         # filter for movies associated with current genre. Calculate number of observations
         n_residual_for_genre_movies <- train[str_detect(train$genres, pattern=genre), .N]
         
         # output
         data.frame(genre = genre, mean_residual = mean_residual_for_genre_movies, sd_residual = sd_residual_for_genre_movies, n_residual = n_residual_for_genre_movies)
         }
      )
	  
mean_residual_per_genre <- rbindlist(mean_residual_per_genre)


# plot boxplots of average movie ratings by associated genre:
mean_residual_per_genre %>%
  mutate(ci = sd_residual/sqrt(n_residual) * qnorm(0.975)) %>%
  mutate(genre = reorder(genre, -mean_residual)) %>%
  filter(genre != "nogenreslisted") %>% # exclude outlier genre with only n = 6 ratings
  ggplot(aes(x=genre, y=mean_residual)) +
  theme(axis.text.x = element_text(angle=-90, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_residual + ci, ymin = mean_residual - ci)) +
  geom_hline(yintercept=0, linetype="dashed", color = "red") +
#  geom_hline(yintercept=sd(train$residuals_m_u_effect_model) / sqrt(nrow(train)) * qnorm(0.975), color = "blue") +
#  geom_hline(yintercept=-sd(train$residuals_m_u_effect_model) / sqrt(nrow(train)) * qnorm(0.975), color = "blue") +
  labs(title = "95% CIs of Mean Residuals (Movie & User Effect Model)",
       subtitle = "by Associated Genre") +
  ylab("Residual")
```
In the plot above, the red dashed line is the mean residual. The result suggests that our previous belief that the 'horror' genre _generally_ receives the worst ratings was wrong. After having accounted for move and user effects, it turns out that the 'horror' genre performs above average, and 'children' movies rank worst, while documentaries rank best.

From the data exploration section, we know that _one_ movie can be associated with _many_ different genres. To capture the genre-to-genre variability, for each genre, I am introducing one binary variable $D_g$ which is 1 if a movie is associated with the specific genre, and 0 otherwise. Using the `r text_spec("lm()", color="black", background="#EEEEEE")`-function it is then possible to fit the movie and user effect model's residuals to the genre dummy variables $D_g$. The least squares estimation conviniently accounts for the possibility of overlapping movie genres. The predictions of this model, $\beta^g$, are then becoming the variable to capture the 'genre' bias. Hence, $\beta^g$ is a function of $D^{\text{Action}}$, $D^{\text{Comedy}}$, ..., $D^{\text{War}}$.

I am considering dummy variables for all associated movie genres having _narrow_ and _non-overlapping 95% CIs_ with the movie and user effect residual's CI. As seen in the plot above, my inclusion criteria is matched by all genres except for 'IMAX'. The group 'nogenreslisted' is excluded from the plot above and has very large, overlapping CIs due to small sample size. Hence, I am considering all genres except for 'IMAX' and 'no_genres_listed', which leaves 18 dummy variables.

```{r model-beta-g-hat}
# define dummy variable names for all genres except IMAX and no_genres_listed
genre_dummy_vars <- single_genres[which(!single_genres %in% c("IMAX", "nogenreslisted"))]

# create dummy variables. Assign zero value for all
train[, (genre_dummy_vars) := 0]

# for each dummy variable, change value to 1 whenever a movie is associated with the dummy's genre
for (dummy in genre_dummy_vars) {
  train[str_detect(genres, dummy), (dummy) := 1]}


## add beta_g_hat to data

# create dummy vars on test set
test[, (genre_dummy_vars) := 0]
for (dummy in genre_dummy_vars) {
  test[str_detect(genres, dummy), (dummy) := 1]}
  
# use lm to include dummy vars and create prediction 'beta_g_hat'
vars <- c("residuals_m_u_effect_model", genre_dummy_vars)
fit_genre_dummies <- lm(residuals_m_u_effect_model ~ ., data = train[, vars, with = FALSE])
kable(summary(fit_genre_dummies)$coef, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```
As expected, movies associated with genres 'Documentary' and 'Children' have the greatest impact on the movie-user-effects models' residuals.

```{r predict-beta-g-hat}
# predict beta_g_hat on data
train[, beta_g_hat := fit_genre_dummies$fitted.values]
test[, beta_g_hat := predict(fit_genre_dummies, newdata = test)]

# predict y for movie, user and genre effect model on test set
y_hat <- benchmark_model + test$beta_m_hat + test$beta_u_hat + test$beta_g_hat

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

# obtain RMSE on test set
rmse_m_u_g_effect <- get_rmse(y_hat = y_hat, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "Movie, User, Genre Effects",
                     Variables = "$\\mu, \\beta_m, \\beta_u, \\beta_g$", 
                     RMSE = rmse_m_u_g_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

Observe that adding the 'genre' effect to the movie and user effect model improves RMSE performance on the test set from `r round(models[3,3],5)` to `r round(models[4,3],5)`. Albeit modest gains in the performance measure, statistical significance suggests that these variables are part of the model. Going forward, I will build upon the residuals of the movie, user and genre effect model.

### User-Specific Genre Variability

In the 'user' effect model, the user's bias for general movie ratings is accounted for. For one user, this variable $\beta_u$ is constant across all movies he or she has rated. It captures the effect whether a person is _generally_ rather 'easy to impress' or more critical of a movie's quality.

Likewise, the previously created variable $\widehat{\beta_g}$ only captures the general bias against a specific genre. We saw that, after having accounted for the movie quality (movie effect) and user bias (user effect), documentaries generally receive better ratings than movies for children.

So far, only these 'general' effects have been investigated. However, it is reasonable to expect that a specific individual has a _unique preference_ for movies of specific genre types. For instance, a person might adores 'romantic' movies, while he/she dislikes 'horror' movies. To account for this _user-specific_ genre effect, I am introducing variable $\beta^g_{u}$. Working from the residuals of the movie, user and genre effect model, for each genre $g$ I am calculating the mean residual for user $u$. Since there are 20 different single genres, there will be 20 such variables $[\beta^{\text{Action}}_u, \beta^{\text{Adventure}}_u,\beta^{\text{Animation}}_u, ..., \beta^{\text{Western}}_u]$.

Analog to the previous section, I am then using the `r text_spec("lm()", color="black", background="#EEEEEE")`-function to predict one variable capturing the user-specific genre bias: $\beta^g_u$.

```{r user-specific-genre-effect}
library(data.table)
library(tidyverse)

# work from residuals of the movie-user-genre effect model
train <- cbind(train, residuals_m_u_g_effect_model = fit_genre_dummies$residuals)

# user-specific genre effect
for (genre in genre_dt$genre){
  # filter for movies associated with current genre
  # Calculate mean residual of movie-user-genre effect model
  train[str_detect(genres, genre), (paste0("beta_", genre, "_u_hat")) := mean(residuals_m_u_g_effect_model), userId]
}

# for all variables, set obs without associated genre to 0
invisible(lapply(names(train),function(.name) set(train, which(is.na(train[[.name]])), j = .name,value = 0)))

# select residuals of movie-user-genre effect model and user-specific genre bias variables
vars <- c("residuals_m_u_g_effect_model", names(train)[str_detect(names(train), "^beta_[A-Za-z]*_u_hat$")])

# create beta_g_u_hat using lm function
fit_user_specific_genre_effect <- lm(residuals_m_u_g_effect_model ~ ., data = train[, vars, with = FALSE])
kable(summary(fit_user_specific_genre_effect)$coef, escape = TRUE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

The t values indicate that each _genre-specfifc_ $\hat\beta_u$ is part of the model (p-val <1%).

```{r user-specific-genre-effect-RMSE}
## add beta_g_u_hat to data

# beta_g_u_hat vars on train set
beta_g_u_hat_vars <- names(train)[str_detect(names(train), "^beta_[A-Za-z]*_u_hat$")]

# create user-specfic genre effect on test set using train set's estimates
# for each genre a movie is associated with, attach the user specific bias from the train set
for (g_u_effect in beta_g_u_hat_vars){
  merge_vars <- c(g_u_effect, "userId")
  genre <- str_split(g_u_effect, "_")[[1]][2]
  current_genre_dt <- merge(test[str_detect(genres, genre), .(userId, movieId)], unique(train[str_detect(genres, genre), merge_vars, with = FALSE]), by = "userId", all.x=TRUE)
  test <- merge(test, current_genre_dt, by = c("userId", "movieId"), all.x=TRUE)
}

# replace NA values with 0
invisible(lapply(names(test),function(.name) set(test, which(is.na(test[[.name]])), j = .name,value = 0)))

# predict beta_u_g_hat on data
train[, beta_u_g_hat := fit_user_specific_genre_effect$fitted.values]
test[, beta_u_g_hat := predict(fit_user_specific_genre_effect, newdata = test)]

# predict y for movie, user, genre and user-specific effect model on test set
y_hat <- benchmark_model + test$beta_m_hat + test$beta_u_hat + test$beta_g_hat + test$beta_u_g_hat

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

# obtain RMSE on test set
rmse_m_u_g_ug_effect <- get_rmse(y_hat = y_hat, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "M, U, G, User-Specific Genre Effects",
                     Variables = "$\\mu, \\beta_m, \\beta_u, \\beta_g, \\beta^g_u$", 
                     RMSE = rmse_m_u_g_ug_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

Accounting for the _user-specific_ genre effect improves RMSE performance from `r round(models[4,3],5)` to `r round(models[5,3],5)`.

### Time Variability

As seen in the genre-to-genre variability section, after having accounted for some variables, the data dependencies for a specific effect may change. This can occur due to omitted variable bias. Below, I reinvestigate time dependencies from the data exploration section, but working from the residuals of the movie, user, genre and user-specific genre effects. 

```{r time-dependencies-model-residuals}
# add residuals of user-specific effect model to train set
train[, residuals_m_u_g_ug_effect_model := fit_user_specific_genre_effect$residuals]

## stratify by quarter
# stratify average ratings by quarter. Calculate 95% CIs
dt_mean <- train[,mean(residuals_m_u_g_ug_effect_model),quarter]
dt_sd <- train[,sd(residuals_m_u_g_ug_effect_model),quarter]
dt_n <- train[,.N,quarter]
dt <- merge(dt_mean[,.(quarter, mean_residuals_m_u_g_ug_effect_model = V1)], dt_sd[,.(quarter, sd = V1)], by="quarter")
dt <- merge(dt, dt_n, by="quarter")

# plot CIs of average movie residuals_m_u_g_ug_effect_models by quarter:
p1n <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=quarter, y=mean_residuals_m_u_g_ug_effect_model)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_residuals_m_u_g_ug_effect_model + ci, ymin = mean_residuals_m_u_g_ug_effect_model - ci)) +
  ggtitle("95% CIs of Mean Residual by Quarter") +
  labs(subtitle = "Movie, User, Genre, User-Specfific Genre Effect Model") +
  ylab("") +
  theme(plot.title = element_text(size=10),
        plot.subtitle = element_text(size=6))


## stratify by month
# stratify average residuals_m_u_g_ug_effect_models by month. Calculate 95% CIs
dt_mean <- train[,mean(residuals_m_u_g_ug_effect_model),month]
dt_sd <- train[,sd(residuals_m_u_g_ug_effect_model),month]
dt_n <- train[,.N,month]
dt <- merge(dt_mean[,.(month, mean_residuals_m_u_g_ug_effect_model = V1)], dt_sd[,.(month, sd = V1)], by="month")
dt <- merge(dt, dt_n, by="month")

# plot CIs of average movie residuals_m_u_g_ug_effect_models by month:
p2n <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=month, y=mean_residuals_m_u_g_ug_effect_model)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_residuals_m_u_g_ug_effect_model + ci, ymin = mean_residuals_m_u_g_ug_effect_model - ci)) +
  ggtitle("95% CIs of Mean Residual by Month") +
  labs(subtitle = "Movie, User, Genre, User-Specfific Genre Effect Model") +
  ylab("") +
  theme(plot.title = element_text(size=10),
        plot.subtitle = element_text(size=6))


## stratify by week
# stratify average residuals_m_u_g_ug_effect_models by week. Calculate 95% CIs
dt_mean <- train[,mean(residuals_m_u_g_ug_effect_model),week]
dt_sd <- train[,sd(residuals_m_u_g_ug_effect_model),week]
dt_n <- train[,.N,week]
dt <- merge(dt_mean[,.(week, mean_residuals_m_u_g_ug_effect_model = V1)], dt_sd[,.(week, sd = V1)], by="week")
dt <- merge(dt, dt_n, by="week")

# plot CIs of average movie residuals_m_u_g_ug_effect_models by week:
p3n <- dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
  ggplot(aes(x=week, y=mean_residuals_m_u_g_ug_effect_model)) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
  geom_errorbar(aes(ymax = mean_residuals_m_u_g_ug_effect_model + ci, ymin = mean_residuals_m_u_g_ug_effect_model - ci)) +
  ggtitle("95% CIs of Mean Residual by Week") +
  labs(subtitle = "Movie, User, Genre, User-Specfific Genre Effect Model") +
  ylab("") +
  theme(plot.title = element_text(size=10),
        plot.subtitle = element_text(size=6))


# use smoothing for weekly plot
p4n <- dt %>%
  ggplot(aes(x=week, y=mean_residuals_m_u_g_ug_effect_model)) +
  geom_point() +
  geom_smooth(span=0.3, se = FALSE) +
  ggtitle("Mean Residual by Week (LOESS)") +
  labs(subtitle = "Movie, User, Genre, User-Specfific Genre Effect Model") +
  ylab("") +
  theme(plot.title = element_text(size=10),
        plot.subtitle = element_text(size=6))

grid.arrange(p1n, p2n, p3n, p4n, ncol=2)
```
The residuals of the movie, user, genre and user-specfific genre effect model show similar time dependencies to what was discovered in the data exploration section (i.e. prior to accounting for _any_ other variables). The stability of time dependencies is somewhat surprising, as I personally had thought that a user would _not_ rate a specfific movie more favorably in November than in March.  

Note that for the quarterly plot, both 95% confidence intervals for ratings made in the first quarter of the year vs. ratings made in the 4th quarter of the year are neither overlapping with one another nor overlapping with the _mean_ value of zero. This is reasonable evidence to capture a time effect. To keep it simple, I am working from the the movie, user, genre and user-specfic genre effect model's residuals and introduce dummy variables for ratings made in the first and ratings made in the 4th quarter. Then, analog to the previous procedure, I am using the `r text_spec("lm()", color="black", background="#EEEEEE")`-function and the dummy variables to predict the single time-effect variable, $\beta_t$. Hence, $\beta_t$ is a function of dummy variables $D^{Q_1}$ and $D^{Q_4}$.

```{r time-effect-model}
# add dummy variables for ratings made in Q1 and ratings made in Q4
train[, Q1 := 0]
train[quarter == 1, Q1 := 1]
train[, Q4 := 0]
train[quarter == 4, Q4 := 1]

# fit time effect model on user-specific genre effect model's residuals
fit_m_u_g_gu_t_effect <- lm(residuals_m_u_g_ug_effect_model ~ Q1 + Q4, data = train)
kable(summary(fit_m_u_g_gu_t_effect)$coef, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

The binary variable $D^{Q_4}$ is significant at the 1% level, while $D^{Q_1}$ is significant at the 10% level.

```{r RMSE-time-effect-model}
## check RMSE
# add dummies to test data
library(lubridate)
# extract 'quarter', 'month', and 'week' from 'timestamp' variable
test[, timestamp := as_datetime(timestamp)]
test[,(c("quarter", "month", "week")) := list(quarter(timestamp), month(timestamp), week(timestamp))]
test[, Q1 := 0]
test[quarter == 1, Q1 := 1]
test[, Q4 := 0]
test[quarter == 4, Q4 := 1]

# predict beta_t_hat
train[, beta_t_hat := predict(fit_m_u_g_gu_t_effect)]
test[, beta_t_hat := predict(fit_m_u_g_gu_t_effect, newdata = test)]
y_hat <- benchmark_model + test$beta_m_hat + test$beta_u_hat + test$beta_g_hat + test$beta_u_g_hat + test$beta_t_hat

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

# obtain RMSE on test set
rmse_m_u_g_ug_t_effect <- get_rmse(y_hat = y_hat, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "M, U, G, UG, Time Effects",
                     Variables = "$\\mu, \\beta_m, \\beta_u, \\beta_g, \\beta^g_u, \\beta_t$", 
                     RMSE = rmse_m_u_g_ug_t_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```
Note that RMSE performance improves slightly from `r round(models[5,3],7)` to `r round(models[6,3],7)`. Though the performance increase is practically negligible, on the test set it is confirmed that movies are generally rated a bit worse in the first quarter of the year in comparison to the fourth quarter of the year.

The final model captures all of the following; movie, user, genre, user-specific genre and time effects. 

$$Y_{m,u} = \mu + \beta_m + \beta_u + \beta_g(D^{\text{Action}}, D^{\text{Comedy}}, ...) + \beta^g_u(\beta^{\text{Action}}_u, \beta^{\text{Comedy}}_u, ...) + \beta_t(D^{Q_1}, D^{Q_4}) + \epsilon_{m,u}$$

In the next section I investigate sources of errors and attempt to improve RMSE performance by shrinking estimates with few observations towards their mean value (i.e. zero for residuals).

# Error Analysis

```{r error-analysis, echo=TRUE}
# add residuals of time effect model
y_hat <- benchmark_model + train$beta_m_hat + train$beta_u_hat + train$beta_g_hat + train$beta_u_g_hat + train$beta_t_hat
res_train <- train$rating - y_hat
train[, residuals_m_u_g_ug_t_model := res_train]

# compute absolute error
train[, abs_error := abs(residuals_m_u_g_ug_t_model)]

# largest absolute errors and beta values
cols <- c("title", "abs_error", "beta_m_hat", "beta_u_hat", "beta_g_hat", "beta_u_g_hat", "beta_t_hat")
dt <- train[order(-abs_error), ..cols]
dt <- cbind(dt[,.(title)], dt[ , lapply(.SD, round, 4), .SDcols = cols[-1]])
names(dt) <- c("title", "$|\\hat\\epsilon|$", "$\\widehat\\beta_m$", "$\\widehat\\beta_u$", "$\\widehat\\beta_g$", "$\\widehat\\beta^g_u$", "$\\widehat\\beta_t$")
dt[, title := str_trunc(title, 22)]
kable(head(dt, 5), escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")

# smallest absolute errors and betas
kable(tail(dt, 5), escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```
On a first look, it seems that the main prediction's contributors, $\widehat{\beta_m}$, $\widehat{\beta_u}$ and $\widehat{\beta^g_u}$ have larger absolute values for large absolute errors in comparison to the low absolute error scenario. Next, I am comparing the median absolute values of the $\hat\beta$ estimates for the largest $1,000$ absolute errors to the complete train set's values.

```{r error-analysis2}
# calculate median absolute values for betas and the 1,000 largest errors
abs_vals_high_error <- sapply(dt[1:1000, -1], FUN = function(x){
  round(median(abs(x)),3)
})

# calculate median absolute values for betas and all errors
abs_vals_general <- sapply(dt[,-1], FUN = function(x){
  round(median(abs(x)),3)
})

tbl  <- rbind(abs_vals_general,abs_vals_high_error)
tbl <- as.data.table(cbind(c("Median (Train Set)", "Median (Largest Errors)"),tbl))

names(tbl)[1] <- "Summary Statistic"
kable(tbl, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

Except for the time and genre effects, all $\hat\beta$ have larger median absolute values when the model's absolute error is large. Large absolute values of $\hat\beta$ might occur when estimates are based on _few_ observations, resulting in estimates that are not trustworthy.

# Regularization

In the following, I am attempting to shrink values of $\hat\beta$ towards zero based on the number of observations the estimates are computed from. The analysis in the previous section suggests that this is most meaningful for $\hat\beta_m$, $\hat\beta_u$ and $\hat\beta^g_u$. Instead of calculating mean values to estimate any $\beta$, I am calculating the _penalized mean_ value, with penalty parameter $\delta$:

$$\tilde\beta = \frac{1}{N + \delta} \sum_{i=1}^{N} x_i$$

Below I am tuning the $\delta$ parameter for $\hat\beta_m$, $\hat\beta_u$ and $\hat\beta^g_u$.

## Movie Effect

I re-estimate $\beta_m$ where $\hat\beta_m$ is shrunk towards zero, where shrinkage is larger for small number of observations. Below, I am using the test set to tune the penalty parameter $\delta$ and I am considering values $[0, 0.5, 1, ..., 5]$. Note that $\delta = 0$ is equivalent to having no penalty and results in the RMSE previously obtained from the movie effect model.  

```{r tune-penalized-movie-effect}
# start from the residuals of the benchmark model (i.e. de-meaned Y variable)

# record results in data table
outcome <- data.table(delta = NULL, RMSE = NULL)

# consider values for delta from 0 to 10
for (delta in seq(0, 10, 0.5)){
  
  # compute penalized mean residuals per movieId
  mean_rating_per_movie <- train[, .(penalized_beta_m_hat = sum(residuals_benchmark)/(.N + delta)), movieId]
  
  # add (penalized) mean_rating_per_movie, estimated from train set, to test set
  test <- merge(test, mean_rating_per_movie, by="movieId", all.x=TRUE)
  
  # predict movie effect model using test set
  y_hat_test_set <- benchmark_model + test$penalized_beta_m_hat
  
  # get RMSE from test set
  rmse_current_delta <- get_rmse(y_hat = y_hat_test_set, y = test$rating)
  
  # record result
  current_delta_result <- data.frame(delta = delta, RMSE = rmse_current_delta)
  outcome <- rbind(outcome, current_delta_result)
  
  # remove penalized_beta_m_hat from test set
  test[, penalized_beta_m_hat := NULL]
}

# plot results
outcome %>%
  ggplot(aes(x=delta, y=RMSE)) +
  geom_point() +
  ggtitle("Penalized Beta Movie Effect Estimation")
```
As seen in the plot above, the RMSE on the test set is minimized for penalized $\hat\beta_m$ with $\delta = 3$.

```{r compute-rmse-penalized-movie-effect}
## add residuals of penalized movie effect model to train set

# compute penalized beta m hat
train[, penalized_beta_m_hat := sum(residuals_benchmark)/(.N + 3), movieId]

# add penalized beta m hat estimated on train set to test set
test <- merge(test, unique(train[,.(movieId, penalized_beta_m_hat)]), by="movieId", all.x=TRUE)

# compute and add residuals
y_hat_train <- benchmark_model + train$penalized_beta_m_hat
train[, residuals_penalized_movie_effect_model := rating - y_hat_train]

# summarize models' performance
new_model <- data.frame(Model= "Penalized Movie Effect",
                     Variables = "$\\mu, \\tilde\\beta_m$", 
                     RMSE = outcome[delta == 3, RMSE])

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```
As seen above, regularization for the movie effect model slightly improves RMSE performance from `r round(models[2,3],5)` to `r round(models[7,3],5)`. Analog to the procedure in the previous sections, I add other (penalized) effects to the residuals of this model.

## User Effect

```{r penalized-user-effect-delta}
# start from the residuals of the penalized movie effect model

# record results in data table
outcome <- data.table(delta = NULL, RMSE = NULL)

# consider values for delta from 0 to 10
for (delta in seq(0, 10, 0.5)){
  
  # compute penalized mean residuals per userId
  mean_rating_per_user <- train[, .(penalized_beta_u_hat = sum(residuals_penalized_movie_effect_model)/(.N + delta)), userId]
  
  # add (penalized) mean_rating_per_user, estimated from train set, to test set
  test <- merge(test, mean_rating_per_user, by="userId", all.x=TRUE)
  
  # predict user effect model using test set
  y_hat_test_set <- benchmark_model + test$penalized_beta_m_hat + test$penalized_beta_u_hat
  
  # get RMSE from test set
  rmse_current_delta <- get_rmse(y_hat = y_hat_test_set, y = test$rating)
  
  # record result
  current_delta_result <- data.frame(delta = delta, RMSE = rmse_current_delta)
  outcome <- rbind(outcome, current_delta_result)
  
  # remove penalized_beta_u_hat from test set
  test[, penalized_beta_u_hat := NULL]
}

# plot results
outcome %>%
  ggplot(aes(x=delta, y=RMSE)) +
  geom_point() +
  ggtitle("Penalized Beta User Effect Estimation")
```

As seen above, RMSE is minimized for penalized $\beta_u$ estimation with $\delta=5$.

```{r penalized-user-effect-fit}

## add residuals of penalized movie and user effect model to train set

# compute penalized beta u hat
train[, penalized_beta_u_hat := sum(residuals_penalized_movie_effect_model)/(.N + 5), userId]

# add penalized beta u hat estimated on train set to test set
test <- merge(test, unique(train[,.(userId, penalized_beta_u_hat)]), by="userId", all.x=TRUE)

# compute and add residuals
y_hat_train <- benchmark_model + train$penalized_beta_m_hat + train$penalized_beta_u_hat
train[, residuals_penalized_movie_user_effect_model := rating - y_hat_train]

# summarize models' performance
new_model <- data.frame(Model= "Penalized Movie, User Effects",
                     Variables = "$\\mu, \\tilde\\beta_m, \\tilde\\beta_u$", 
                     RMSE = outcome[delta == 5, RMSE])

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")

```
The _penalized_ movie and user effect model performs slightly better than the original movie and user effect model. As noted previously, regularization seems to be sensible for the user-specific genre effect as well. Consistent with the previous procedure, I will first account for the genre effect on the penalized movie and user effect model. Then, I will regularize the _user-specific_ genre effect

```{r penalized-genre-effect-model-fit}
# Work from penalized movie and user effect model's residuals to account for the 'genre' effect
# use lm to include dummy vars and create prediction 'beta_g_hat_new'
vars <- c("residuals_penalized_movie_user_effect_model", genre_dummy_vars)
fit_genre_dummies <- lm(residuals_penalized_movie_user_effect_model ~ ., data = train[, vars, with = FALSE])

#summary(fit_genre_dummies)

# predict beta_g_hat_new on data
train[, beta_g_hat_new := fit_genre_dummies$fitted.values]
test[, beta_g_hat_new := predict(fit_genre_dummies, newdata = test)]

# predict y for penalized movie and user effects and genre effect on test data
y_hat <- benchmark_model + test$penalized_beta_m_hat + test$penalized_beta_u_hat + test$beta_g_hat_new

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

# obtain RMSE on test set
rmse_pm_pu_g_effect <- get_rmse(y_hat = y_hat, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "Penalized Movie, User, Genre Effects",
                     Variables = "$\\mu, \\tilde\\beta_m, \\tilde\\beta_u, \\beta_g$", 
                     RMSE = rmse_pm_pu_g_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

## User-Specfic Genre Effect

As shown previously, each user has an individual bias against movies associated with specific genres. However, a user might only have one rating for a movie associated with the genre 'Horror' while there are many ratings associated with the genre 'Romance'. The mean rating of this user for the horror genre therefore might not be representative of _all_ 'horror'-movies this user is rating in the future. Hence it is meaningful to penalize the user-specific genre effect for small sample sizes.

### Tuning the Penalty Parameter

Regularization of the user-specific genre effect is a complex procedure. Working from the residuals of the genre and penalized movie and user effect model, I am tuning the penalty parameter $\delta$ as follows:

Consider (1) a range of parameter values $\delta$ to penalize user-specific genre effects having few observations, and (2) all 20 single genres a movie can be associated with.

1. fix the penalty parameter $\delta$ to try, i.e. $d_1$
2. fix the genre a movie can be associated with, i.e. $g_1$
3. filter the `r text_spec("train", color="black", background="#EEEEEE")` set for movies associated with genre $g_1$
4. for each `r text_spec("userId", color="black", background="#EEEEEE")`, use $d_1$ to calculate the _penalized_ mean residual
5. the above yields $\tilde\beta^{g_1}_u$ on the `r text_spec("train", color="black", background="#EEEEEE")` set
6. **predict** $\tilde\beta^{g_1}_u$ for the `r text_spec("test", color="black", background="#EEEEEE")` set by merging the `r text_spec("train", color="black", background="#EEEEEE")` set's $\tilde\beta^{g_1}_u$ onto the `r text_spec("test", color="black", background="#EEEEEE")` set
7. repeat steps 2-6 until all genres, i.e. $g_{20}$ is reached
8. now, we have 20 user-specific genre variables, penalized by $d_1$
9. use `r text_spec("lm()", color="black", background="#EEEEEE")` on the `r text_spec("train", color="black", background="#EEEEEE")` set to fit a linear model predicting the model residuals using the penalized user-specific genre effect betas
10. use the fitted model in (8) on the `r text_spec("test", color="black", background="#EEEEEE")` set to predict _one_ variable, $\tilde\beta^g_u(d_1)$
11. use all penalized betas, the genre-effect beta and the benchmark model to predict ratings on the test set
12. record RMSE performance on the test set along with used penalty $d_1$, and redo steps 1 to 11 until the range of $\delta$ is considered

Afterwards, I will plot RMSE performance vs. $\delta$ and choose the best performing penalty to go forward.

```{r tune-delta-for-user-specific-genre-effect}
# work from residuals of the genre effect and penalized movie and user effects model
# compute and add residuals
y_hat_train <- benchmark_model + train$penalized_beta_m_hat + train$penalized_beta_u_hat + train$beta_g_hat_new
train[, residuals_genre_penalized_movie_user_effect_model := rating - y_hat_train]

# record results in data table
outcome <- data.table(delta = NULL, RMSE = NULL)

# consider values for delta from 50 to 100
for (delta in seq(50, 100, 10)){
  
  # fix genre
  for (genre in single_genres){
  
  # define variable name
  current_var_name <- paste0("p",delta,"_beta_u_", genre)
    
  # for movies associated with current genre, compute penalized mean residuals per userId
  train[str_detect(genres, genre), (current_var_name) := sum(residuals_genre_penalized_movie_user_effect_model)/(.N + delta), userId]
  
  # replace NAs by zero, i.e. where movies are not associated with current 'genre'
  train[is.na(get(current_var_name)), (current_var_name) := 0]
  
  # add (penalized) residual per user, estimated on train set, to test set
  vars <- c("userId", current_var_name)
  test <- merge(test, unique(train[get(current_var_name) != 0, vars, with = FALSE]), by="userId", all.x=TRUE)
  test[!str_detect(genres, genre), (current_var_name) := 0 ] # non-zero values for current 'genre' only
  test[is.na(get(current_var_name)), (current_var_name) := 0] # zero values for genres not rated by the user
  }
  
  # fit user-specific genre effect on train set
  vars <- c("residuals_genre_penalized_movie_user_effect_model",paste0("p",delta,"_beta_u_", single_genres))
  fit_penalized_user_specific_genre_effect <- lm(residuals_genre_penalized_movie_user_effect_model ~., data = train[,vars, with = FALSE])
  
  # predict single variable 'penalized_beta_u_g_hat' on test set
  test[, penalized_beta_g_u_hat := predict(fit_penalized_user_specific_genre_effect, newdata = test)]
  
  # predict rating on test set
  y_hat_test_set <- benchmark_model + test$penalized_beta_m_hat + test$penalized_beta_u_hat + test$beta_g_hat_new + test$penalized_beta_g_u_hat
  
  # restrict predictions to [0.5, 5]
  y_hat_test_set[y_hat_test_set>5] <- 5
  y_hat_test_set[y_hat_test_set<0.5] <- 0.5
  
  # get RMSE from test set
  rmse_current_delta <- get_rmse(y_hat = y_hat_test_set, y = test$rating)
  
  # record result
  current_delta_result <- data.frame(delta = delta, RMSE = rmse_current_delta)
  outcome <- rbind(outcome, current_delta_result)
  
  # remove penalized_beta_g_u_hat and other created variables for current delta
  test[, penalized_beta_g_u_hat := NULL]
  removal_vars <- paste0("p",delta,"_beta_u_", single_genres)
  train <- train[, names(train)[!names(train) %in% removal_vars], with = FALSE]
  test <- test[, names(test)[!names(test) %in% removal_vars], with = FALSE]
  gc()
}

# plot results
outcome %>%
  ggplot(aes(x=delta, y=RMSE)) +
  geom_point() +
  ggtitle("Penalized User-Specific Genre Effect Estimation")

```
A $\delta$ of 70 minimizes RMSE on the `r text_spec("test", color="black", background="#EEEEEE")` set, yielding an RMSE of `r round(outcome[which(outcome[,1] == 70),]$RMSE,4)`.

```{r penalized-user-specific-genre-effect-fit}
# summarize models' performance
new_model <- data.frame(Model= "Penalized M, U, G, User-Specific Genre Effects",
                     Variables = "$\\mu, \\tilde\\beta_m, \\tilde\\beta_u, \\beta_g, \\tilde\\beta^g_u$", 
                     RMSE = outcome[which(outcome[,1] == 70),]$RMSE)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

### Time Dependencies of Model's Residuals

Lastly, I am analyzing the penalized movie, user and user-specific genre effect and general genre effect model's residuals for quarterly time dependencies.

```{r user-specific-genre-effect-time-dependencies}
## add residuals
delta <- 70

  # fix genre
  for (genre in single_genres){
  
  # define variable name
  current_var_name <- paste0("p",delta,"_beta_u_", genre)
    
  # for movies associated with current genre, compute penalized mean residuals per userId
  train[str_detect(genres, genre), (current_var_name) := sum(residuals_genre_penalized_movie_user_effect_model)/(.N + delta), userId]
  
  # replace NAs by zero, i.e. where movies are not associated with current 'genre'
  train[is.na(get(current_var_name)), (current_var_name) := 0]
  
  # add (penalized) residual per user, estimated on train set, to test set
  vars <- c("userId", current_var_name)
  test <- merge(test, unique(train[get(current_var_name) != 0, vars, with = FALSE]), by="userId", all.x=TRUE)
  test[!str_detect(genres, genre), (current_var_name) := 0 ] # non-zero values for current 'genre' only
  test[is.na(get(current_var_name)), (current_var_name) := 0] # zero values for genres not rated by the user
  }

# fit user-specific genre effect on train set
vars <- c("residuals_genre_penalized_movie_user_effect_model",paste0("p",delta,"_beta_u_", single_genres))
fit_penalized_user_specific_genre_effect <- lm(residuals_genre_penalized_movie_user_effect_model ~., data = train[,vars, with = FALSE])

# predict penalized_beta_user_specific_g_effect on test set
test[, p_beta_m_u_ug_g := predict(fit_penalized_user_specific_genre_effect, newdata=test)]

## add residuals to train set
train[, p_beta_m_u_ug_g := fit_penalized_user_specific_genre_effect$fitted.values]
y_hat_train <- benchmark_model + train$penalized_beta_m_hat + train$penalized_beta_u_hat + train$beta_g_hat_new + train$p_beta_m_u_ug_g

# restrict predictions to [0.5, 5]
y_hat_train[y_hat_train > 5] <- 5
y_hat_train[y_hat_train < 0.5] <- 0.5

# add residuals
residuals <- train$rating - y_hat_train
train[, residuals_penalized_user_specfic_genre_effect := residuals]

# investigate time dependencies on residuals
# stratify average ratings by quarter. Calculate 95% CIs
dt_mean <- train[,mean(residuals_penalized_user_specfic_genre_effect),quarter]
dt_sd <- train[,sd(residuals_penalized_user_specfic_genre_effect),quarter]
dt_n <- train[,.N,quarter]
dt <- merge(dt_mean[,.(quarter, mean_residuals_penalized_user_specfic_genre_effect = V1)], dt_sd[,.(quarter, sd = V1)], by="quarter")
dt <- merge(dt, dt_n, by="quarter")

# plot CIs of average movie residuals_penalized_user_specfic_genre_effects by quarter:
dt %>%
  mutate(ci = sd/sqrt(N) * qnorm(0.975)) %>%
#  mutate(genre = reorder(genre, -mean_movie_residuals_penalized_user_specfic_genre_effects)) %>%
  ggplot(aes(x=quarter, y=mean_residuals_penalized_user_specfic_genre_effect)) +
#  geom_text(aes(label=round(mean_residuals_penalized_user_specfic_genre_effect,1)), vjust=-0.5) +
  theme(axis.text.x = element_text(angle=0, hjust=0, vjust=0.2)) +
#  ylim(0, max_n_movies + 100) +
  geom_errorbar(aes(ymax = mean_residuals_penalized_user_specfic_genre_effect + ci, ymin = mean_residuals_penalized_user_specfic_genre_effect - ci)) +
  ggtitle("95% CIs of Mean Residual by Quarter") +
  labs(subtitle = "Penalized User-Specfific Genre Effect Model") +
  ylab("Residual")
```
Even after accounting for the penalized movie, user and user-specific genre effects, there is virtually the same time dependency pattern present in the data as observed previously. As before, I capture time effects via the introduction of dummy variables for ratings made in Q1 and Q4 of the year.

```{r fit-penalized-time-effect-model}
# fit time effect model on user-specific genre effect model's residuals
fit_pm_u_g_gu_t_effect <- lm(residuals_penalized_user_specfic_genre_effect ~ Q1 + Q4, data = train)
kable(summary(fit_pm_u_g_gu_t_effect)$coef, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

The dummy variables' significance indicates robustness across model variants.

```{r fit-penalized-time-effect-model-rmse}
## check RMSE
# predict beta_t_hat
train[, beta_t_hat_new := fit_pm_u_g_gu_t_effect$fitted.values]
test[, beta_t_hat_new := predict(fit_pm_u_g_gu_t_effect, newdata = test)]
y_hat <- benchmark_model + test$penalized_beta_m_hat + test$penalized_beta_u_hat + test$beta_g_hat_new + test$p_beta_m_u_ug_g + test$beta_t_hat_new

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

# obtain RMSE on test set
rmse_m_u_g_ug_t_effect <- get_rmse(y_hat = y_hat, y = test$rating)

# summarize models' performance
new_model <- data.frame(Model= "Penalized M, U, G, UG, Time Effects",
                     Variables = "$\\mu, \\tilde\\beta_m, \\tilde\\beta_u, \\beta_g, \\tilde\\beta^g_u, \\beta_t$", 
                     RMSE = rmse_m_u_g_ug_t_effect)

names(new_model)[3] <- "$\\text{RMSE}_{\\text{test}}$"
models <- rbind(models, new_model)
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

As seen previously, capturing the time effect slightly improves RMSE performance.

# Final Model and Validation Set Result

The final model yields the lowest RMSE of `r round(rmse_m_u_g_ug_t_effect,6)` on the `r text_spec("test", color="black", background="#EEEEEE")` set. It captures all of the following;

* mean rating ('benchmark model')
* movie-to-user variablity via (penalized) $\tilde\beta_m$, with $\delta = 3$
* user-to-user variability via (penalized) $\tilde\beta_m$, with $\delta = 5$
* genre-to-genre variability via $\beta_g$
* user-specific genre variability via (penalized) $\tilde\beta^g_u$, with $\delta = 70$
* time variability via $\beta_t$

$$Y_{m,u} = \mu + \tilde\beta_m + \tilde\beta_u + \beta_g(D^{\text{Action}}, D^{\text{Comedy}}, ...) + \tilde\beta^g_u(\beta^{\text{Action}}_u, \beta^{\text{Comedy}}_u, ...) + \beta_t(D^{Q_1}, D^{Q_4}) + \epsilon_{m,u}$$

Going forward, I fit the model above on the _complete_ `r text_spec("edx", color="black", background="#EEEEEE")` dataset. Using this information and the new X-variables of the `r text_spec("validation", color="black", background="#EEEEEE")` set, I predict `r text_spec("rating", color="black", background="#EEEEEE")` on the `r text_spec("validation", color="black", background="#EEEEEE")` set, yielding the final 'validation set' RMSE performance.

```{r final-model-validation-RMSE, echo=TRUE}
### fit model using edx set

## 1 mean rating
mean_rating <- mean(edx$rating)
edx[, residuals_mean := rating - mean_rating]


## 2 penalized movie-to-movie variability
edx[, penalized_beta_m_hat := sum(residuals_mean)/(.N + 3), movieId]
y_hat_edx <- mean_rating + edx$penalized_beta_m_hat # in-sample prediction
edx[, residuals_m_effect := rating - y_hat_edx] # in-sample residuals


## 3 penalized user-to-user variability
edx[, penalized_beta_u_hat := sum(residuals_m_effect)/(.N + 5), userId]
y_hat_edx <- mean_rating + edx$penalized_beta_m_hat + edx$penalized_beta_u_hat # in-sample prediction
edx[, residuals_m_u_effect := rating - y_hat_edx] # in-sample residuals


## 4 genre-to-genre variability
# exclude/change special characters from 'genres'
edx[, genres := str_replace_all(genres, "[-()]", "")]
edx[, genres := str_replace_all(genres, " ", "")]

# define dummy var names
genre_dummy_vars <- single_genres[which(!single_genres %in% c("IMAX", "nogenreslisted"))]

# create dummy variables
edx[, (genre_dummy_vars) := 0]
for (dummy in genre_dummy_vars) {
  edx[str_detect(genres, dummy), (dummy) := 1]}

# fit lm() on edx set to capture genre dummy vars
variables <- c(genre_dummy_vars, "residuals_m_u_effect")
fit_genre_effect <- lm(residuals_m_u_effect ~., data = edx[, variables, with = FALSE])

# create variable beta_g_hat
edx[, beta_g_hat := fit_genre_effect$fitted.values]

# add model residuals
y_hat_edx <- mean_rating + edx$penalized_beta_m_hat + edx$penalized_beta_u_hat + edx$beta_g_hat # in-sample prediction
edx[, residuals_m_u_g_effect := rating - y_hat_edx] # in-sample residuals


## 5 penalized user-specific genre effect
# for each genre, add user-specific effect
delta <- 70
  for (genre in single_genres){
    current_var_name <- paste0("p",delta,"_beta_u_", genre) # define variable name
    
    # for current genre, compute penalized mean residuals per userId
    edx[str_detect(genres, genre), (current_var_name) := sum(residuals_m_u_g_effect)/(.N + delta), userId]
    
    # where movies are not associated with current 'genre' replace NAs by zero
    edx[is.na(get(current_var_name)), (current_var_name) := 0]
  }

# fit user-specific genre effect
vars <- c("residuals_m_u_g_effect",paste0("p",delta,"_beta_u_", single_genres))
fit_user_specific_genre_effect <- lm(residuals_m_u_g_effect ~., data = edx[,vars, with = FALSE])

# create variable beta_ug_hat
edx[, beta_ug_hat := fit_user_specific_genre_effect$fitted.values]

# add model residuals
y_hat_edx <- mean_rating + edx$penalized_beta_m_hat + edx$penalized_beta_u_hat + edx$beta_g_hat + edx$beta_ug_hat # in-sample prediction
edx[, residuals_m_u_g_ug_effect := rating - y_hat_edx] # in-sample residuals


## 5 time variability
# extract 'quarter' from 'timestamp' variable
library(lubridate)
edx[, timestamp := as_datetime(timestamp)]
edx[, quarter := quarter(timestamp)]

# add Q1 and Q4 dummies
edx[, Q1 := 0]
edx[quarter == 1, Q1 := 1]
edx[, Q4 := 0]
edx[quarter == 4, Q4 := 1]

# fit time effect model on user-specific genre effect model's residuals
fit_time_effect <- lm(residuals_m_u_g_ug_effect ~ Q1 + Q4, data = edx)

# create variable beta_t_hat
edx[, beta_t_hat := fit_time_effect$fitted.values]


### use fitted model (edx) and new x-variables of validation set to predict rating on validation set

### 1 add penalized_beta_m_hat (fitted on edx set) to validation set
validation <- merge(validation, unique(edx[,.(penalized_beta_m_hat, movieId)]), by="movieId", all.x = TRUE)


### 2 add penalized_beta_u_hat (fitted on edx set) to validation set
validation <- merge(validation, unique(edx[,.(penalized_beta_u_hat, userId)]), by="userId", all.x = TRUE)


### 3 add beta_g_hat (fitted on edx set) to validation set
## a. add genre dummy variables to validation set
# exclude/change special characters from 'genres' variable
validation[, genres := str_replace_all(genres, "[-()]", "")]
validation[, genres := str_replace_all(genres, " ", "")]

# create dummy variables. Assign zero value for all
genre_dummy_vars <- single_genres[which(!single_genres %in% c("IMAX", "nogenreslisted"))]
validation[, (genre_dummy_vars) := 0]

# for each dummy variable, change value to 1 whenever a movie is associated with the dummy's genre
for (dummy in genre_dummy_vars) {
  validation[str_detect(genres, dummy), (dummy) := 1]}

## b. predict beta_g_hat (fitted on edx set) on validation set
beta_g_hat <- predict(fit_genre_effect, newdata = validation[, genre_dummy_vars, with = FALSE])
validation[, beta_g_hat := beta_g_hat]


### 4 add beta_ug_hat (fitted on edx set) to validation set
# for each genre, add beta_u_<genre>_hat (fit on edx set) to validation set
for (genre in single_genres){
  current_var_name <- paste0("p",delta,"_beta_u_", genre) # define variable name
  vars <- c("userId", current_var_name)
  validation <- merge(validation, unique(edx[get(current_var_name) != 0, vars, with = FALSE]), by="userId", all.x=TRUE)
  validation[!str_detect(genres, genre), (current_var_name) := 0 ] # non-zero values for current 'genre' only
  validation[is.na(get(current_var_name)), (current_var_name) := 0] # zero values for genres not rated by the user
}

# predict beta_ug_hat (fit on edx set) on validation set
beta_u_genre_vars <- paste0("p",delta,"_beta_u_", single_genres)
beta_ug_hat <- predict(fit_user_specific_genre_effect, newdata = validation[, beta_u_genre_vars, with = FALSE])
validation[, penalized_beta_ug_hat := beta_ug_hat]


### 5 add beta_t_hat (fitted on edx set) to validation set
# extract 'quarter' from 'timestamp' variable
validation[, timestamp := as_datetime(timestamp)]
validation[, quarter := quarter(timestamp)]

# add Q1 and Q4 dummies (see report)
validation[, Q1 := 0]
validation[quarter == 1, Q1 := 1]
validation[, Q4 := 0]
validation[quarter == 4, Q4 := 1]

# create variable beta_t_hat
validation[, beta_t_hat := predict(fit_time_effect, newdata = validation[,.(Q1, Q4)])]


### predict validation set's rating
y_hat <- mean_rating + validation$penalized_beta_m_hat + validation$penalized_beta_u_hat + validation$beta_g_hat + validation$penalized_beta_ug_hat + validation$beta_t_hat

# restrict predictions to [0.5, 5]
y_hat[y_hat>5] <- 5
y_hat[y_hat<0.5] <- 0.5

### RMSE on validation set
final_rmse <- sqrt(mean((validation$rating - y_hat)**2))

# summarize models' performance
models <- cbind(models,RMSE_validation=c(rep("",10),round(final_rmse,7)))
names(models)[4] <- "$\\text{RMSE}_{\\text{validation}}$"
                     
kable(models, escape = FALSE, booktabs=TRUE) %>%
  kable_styling(position = "center")
```

The final model (fitted on the `r text_spec("edx", color="black", background="#EEEEEE")` set and applied on the `r text_spec("validation", color="black", background="#EEEEEE")` set) yields an RMSE of `r final_rmse`, i.e. well below the required RMSE of 0.8649 to score full marks on the HarvardX MovieLens project.

# Conclusion and Potential Improvements

The final model is a linear combination of movie, user, genre and time specific effects. While for movie and user specific effects it is straightforward to compute mean ratings per movieId or userId, the genre and the genre-specific user effects are more complex to capture. This is due to the fact that _one_ movie can be associated with _many_ genres at once. Although the `r text_spec("lm()", color="black", background="#EEEEEE")` function partly accounts for this in terms of varying parameter estimates, a more flexible ensemble of trees might improve the predictive power for the genre and the user-specific genre effect. However, the given dataset is large and fitting / tuning more flexible methods is computationally costly. For this project I restricted computational resources to the equivalent of a typical personal computer.

Furthermore, the time dependencies in the data might be better captured by using a smooth function instead of two dummy variables for the quarter of the year. However, the dataset shows that a time effect is significant but has only modest predictive power. A practical improvement by introducing a more complex model to capture time dependencies is questionable for the time effect.